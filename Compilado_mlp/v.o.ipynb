{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL PROCESAMIENTO SISTEMATICO CERTIFICADOS ANALITICOS MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESAMIENTO INFO CERTIFICADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"@author: Francisco Donaire\"\"\"\n",
    "\n",
    "#IMPORTAMOS LAS LIBRERIAS NECESARIAS\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "directory_path = 'G:\\\\.shortcut-targets-by-id\\\\1ugEph6zRyoKHbwpiYuXbB3Pj0UXueIKJ\\\\09. Contrato CS115 EIA 2024\\\\03.WIP\\\\03.Hidroquímica\\\\2. Actualizacion BDD\\\\Certificados Revisados\\\\Otros certificados\\\\Version4'\n",
    "\n",
    "\n",
    "data_certificados = pd.DataFrame()\n",
    "\n",
    "excel_files = [file for file in os.listdir(directory_path) if file.endswith(('.xlsx'))]\n",
    "\n",
    "# Procesar cada archivo Excel encontrados\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(directory_path, file)  # Construye la ruta completa al archivo\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    wb = openpyxl.load_workbook(file_path)\n",
    "\n",
    "    #DETERMINAMOS LA EXTENSION HORIZONTAL DE MANERA AUTOMATICA\n",
    "    for hoja_archivo in xls.sheet_names:\n",
    "        hoja = wb[hoja_archivo]\n",
    "        rango = hoja['C133:AZ133']\n",
    "\n",
    "        ultima_celda_con_datos = None\n",
    "        for celda in rango[0]:\n",
    "            if celda.value is not None:\n",
    "                ultima_celda_con_datos = celda\n",
    "\n",
    "        if ultima_celda_con_datos:\n",
    "            columna = ''.join(filter(str.isalpha, ultima_celda_con_datos.coordinate))\n",
    "            print(f'La última columna con datos es {columna}, en la hoja: {hoja_archivo}, del archivo: {file}')\n",
    "    \n",
    "    \n",
    "            data_identificacion = pd.read_excel(xls, sheet_name= hoja_archivo, skiprows=132, nrows=8, usecols=f\"C:{columna}\")\n",
    "\n",
    "\n",
    "            data_identificacion_transposed = data_identificacion.transpose()\n",
    "\n",
    "\n",
    "            data_identificacion_transposed.columns = data_identificacion_transposed.iloc[0]\n",
    "            data_identificacion_transposed = data_identificacion_transposed.drop(data_identificacion_transposed.index[0])\n",
    "            \n",
    "            # ASIGNACION DE NOMBRES A LAS COLUMNAS\n",
    "            data_identificacion_transposed.columns = ['Fecha', 'Id_Certificado', 'Comentarios', 'Año_QAQC', 'Fecha_QAQC', 'Mes_QAQC', 'Muestras_Con_Datos', 'Muestras_Totales']\n",
    "            data_identificacion_transposed['Index_Column'] = data_identificacion_transposed.index\n",
    "\n",
    "            # RESET DEL INDICE PARA OBTENER UNO NUEVO E INCREMENTAL\n",
    "            data_identificacion_transposed.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            # AGREGA COLUMNA CON LA RUTA DEL ARCHIVO\n",
    "            data_identificacion_transposed['Ruta_Archivo'] = file_path\n",
    "            data_identificacion_transposed.columns = ['Fecha', 'Id_Certificado', 'Comentarios', 'Año_QAQC', 'Fecha_QAQC', 'Mes_QAQC', 'Muestras_Con_Datos', 'Muestras_Totales', 'Estacion', 'Ruta_Archivo']\n",
    "            \n",
    "            # ORDEN DE LA COLUMNAS \n",
    "            data_identificacion_transposed = data_identificacion_transposed[['Estacion','Fecha', 'Id_Certificado', 'Ruta_Archivo', 'Comentarios', 'Año_QAQC', 'Fecha_QAQC', 'Mes_QAQC', 'Muestras_Con_Datos', 'Muestras_Totales']]\n",
    "            \n",
    "            # SE LIMPIA COLUMNA \"ESTACION\"\n",
    "            data_identificacion_transposed['Estacion'] = data_identificacion_transposed['Estacion'].str.replace(r'\\..*$', '', regex=True)\n",
    "            \n",
    "            # SE UNE LA DATA NUEVA AL DATAFRAME PRINCIPAL\n",
    "            data_certificados = pd.concat([data_certificados, data_identificacion_transposed], ignore_index=True)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print(f'No hay datos en el rango C133:AZ133, para la hoja: {hoja_archivo}, para el archivo: {file}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPRIMIR DATA PARA REVISAR CANTIDAD DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_certificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESAR DATA PARAMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "\n",
    "directory_path = 'G:\\\\.shortcut-targets-by-id\\\\1ugEph6zRyoKHbwpiYuXbB3Pj0UXueIKJ\\\\09. Contrato CS115 EIA 2024\\\\03.WIP\\\\03.Hidroquímica\\\\2. Actualizacion BDD\\\\Certificados Revisados\\\\Otros certificados\\\\Version4'\n",
    "\n",
    "data_parametros = pd.DataFrame()\n",
    "\n",
    "# ENCONTRAR ARCHIVOS EXCEL EN EL DIRECTORIO\n",
    "excel_files = [file for file in os.listdir(directory_path) if file.endswith(('.xlsx'))]\n",
    "\n",
    "# LEER CADA ARCHIVO EXCEL ENCONTRADO\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(directory_path, file)  # CONSTRUYE LA RUTA COMPLETA AL ARCHIVO\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    wb = openpyxl.load_workbook(file_path)\n",
    "    # EVALUA COLUMNAS CON DATOS EN EL RANGO\n",
    "    for hoja_archivo in xls.sheet_names:\n",
    "        hoja = wb[hoja_archivo]\n",
    "        rango = hoja['C133:AZ133']\n",
    "        # DETERMINA LA ULTIMA COLUMNA CON DATOS\n",
    "        ultima_celda_con_datos = None\n",
    "        for celda in rango[0]:\n",
    "            if celda.value is not None:\n",
    "                ultima_celda_con_datos = celda\n",
    "        # SI HAY DATOS EN EL RANGO OBTIENE LA COORDENADA DE LA ULTIMA COLUMNA\n",
    "        if ultima_celda_con_datos:\n",
    "            columna = ''.join(filter(str.isalpha, ultima_celda_con_datos.coordinate))\n",
    "            print(f'La última columna con datos es >>> {columna} <<< , en la hoja: >>> {hoja_archivo} <<<')\n",
    "    \n",
    "            # SE LEE EL RANGO DE CELDAS CON LOS DATOS DE LOS PARAMETROS\n",
    "            data_df = pd.read_excel(xls, sheet_name= hoja_archivo, skiprows=141, nrows=111, usecols=f\"B:{columna}\", header=None) #\n",
    "            # data_df.columns = data_df.reset_index()\n",
    "            data_df.drop(data_df.columns[1], axis=1, inplace=True) #ELIMINA COLUMNA VACIA\n",
    "            \n",
    "            # data_df.reset_index(drop=False, inplace=False)\n",
    "            #PIVOTEO DEL DATAFRAME\n",
    "            data_pivoted = data_df.T\n",
    "\n",
    "            # PARAMETROS COMO COLUMNAS, PRIMERA FILA COMO ENCABEZADO\n",
    "            new_header = data_pivoted.iloc[0]  \n",
    "            data_pivoted = data_pivoted[1:]  \n",
    "            data_pivoted.columns = new_header  \n",
    "\n",
    "            #RESETEA EL INDICE\n",
    "            data_pivoted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # SE UNE LA NUEVA DATA \n",
    "            data_parametros = pd.concat([data_parametros, data_pivoted], ignore_index=True)\n",
    "            # for column in data_parametros.columns:\n",
    "            #     data_parametros[column] = pd.to_numeric(data_parametros[column], errors='coerce')\n",
    "            \n",
    "        else:\n",
    "            print(f'No hay datos en el rango C133:AZ133, para la hoja: {hoja_archivo}')\n",
    "            \n",
    "         \n",
    "data_parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISAR DATA PARAMETROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE JUNTAN AMBOS DATA_FRAME PARA LOGRAR FORMATO COLUMNAR INDEXADO|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([data_certificados, data_parametros], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE LIMPIA COLUMNA ESTACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged['Estacion'] = df_merged['Estacion'].str.replace(r'\\..*$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREAMOS CONECCION  A LA TABLA EN LA BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Credenciales db\n",
    "host = 'localhost'\n",
    "database = 'certificados_dbo'\n",
    "username = 'postgres'  \n",
    "password = '1234' \n",
    "\n",
    "# Crea la coneccion\n",
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    database=database,\n",
    "    user=username,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "engine = create_engine('postgresql://'+username+':'+password+'@'+host+'/'+database)\n",
    "table_name = 'certificados_procesados'\n",
    "\n",
    "df_data_bd = pd.read_sql_table('certificados_procesados', con=engine)#Trae la tabla de la bda un dataframe\n",
    "\n",
    "df_data_bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE UNE LA DATA PROVENIENTE DE LA BBDD A LA DATA NUEVA QUE SE ACTUALIZARÁ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_update = pd.concat([df_data_bd, df_merged], ignore_index=True, axis=0) # se le añade la nueva info\n",
    "df_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGAMOS A LA BBDD LA NUEVA DATA COMPILADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_update = df_merged #DESCOMENTAR PARA SOLO INSERTAR UNA TABLA NUEVA\n",
    "\n",
    "df_update.to_sql(table_name, engine, index=False, if_exists='replace',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAJAMOS LA DATA DE LA BBDD A UN DATA-FRAME PARA REVISAR CANTIDADES Y CONSISTENCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'certificados_procesados'\n",
    "\n",
    "df_data_bd_rev = pd.read_sql_table('certificados_procesados', con=engine)  #Trae la tabla de la bda un dataframe\n",
    "\n",
    "\n",
    "df_data_bd_rev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
